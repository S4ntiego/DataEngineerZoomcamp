1. Create a bucket:
```code
terraform apply
```

Big Query Dataset: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset

Add Google Big Query dataset to main.tf:
```terraform main.tf
resource "google_bigquery_dataset" "demo_dataset" {
  dataset_id                  = "demo_dataset"
  location                    = "EU"
}
```

Create new file ```variables.tf```:
```terraform variables.tf
variable "region" {
  description = "Region"
  default     = "europe-west1"
}

variable "project" {
  description = "Project"
  default     = "terraform-demo-423412"
}

variable "location" {
  description = "Project Location"
  default     = "EU"
}

variable "gcs_bucket_name" {
  description = "My Storage Bucket Name"
  default     = "terraform-demo-423412-terra-bucket"
}

variable "bq_dataset_name" {
  description = "My BigQuery Dataset Name"
  default     = "demo_dataset"
}

variable "gcs_storage_class" {
  description = "Bucket Storage Class"
  default     = "STANDARD"
}
```

And change main.tf to include variables:
```terraform main.tf
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "5.29.1"
    }
  }
}

provider "google" {

  project = var.project
  region  = var.region
}

resource "google_storage_bucket" "demo-bucket" {
  #it has to be unique across the whole GCP (so we can use our projectID for example)!!
  name          = var.gcs_bucket_name
  location      = var.location
  force_destroy = true

  lifecycle_rule {
    condition {
      #in days
      age = 1
    }
    action {
      #if the upload action a very big file that was split into chunks wasn't completed during one day - abort the upload
      type = "AbortIncompleteMultipartUpload"
    }
  }
}

resource "google_bigquery_dataset" "demo_dataset" {
  dataset_id = var.bq_dataset_name
  location   = var.location
}
```

```code
unset GOOGLE_CREDENTIALS
```

I have no way of knowing that you can access the project, because I don't have any identification:
```code
terraform plan
```

Add credentials as variable:
```terraform variables.tf
variable "credentials" {
    description = "My Credentials"
    default = "./keys/my-creds.json"
}

variable "region" {
  description = "Region"
  default     = "europe-west1"
}

variable "project" {
  description = "Project"
  default     = "terraform-demo-423412"
}

variable "location" {
  description = "Project Location"
  default     = "EU"
}

variable "gcs_bucket_name" {
  description = "My Storage Bucket Name"
  default     = "terraform-demo-423412-terra-bucket"
}

variable "bq_dataset_name" {
  description = "My BigQuery Dataset Name"
  default     = "demo_dataset"
}

variable "gcs_storage_class" {
  description = "Bucket Storage Class"
  default     = "STANDARD"
}
```

```terraform main.tf
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "5.29.1"
    }
  }
}

provider "google" {
  credentials = file(var.credentials)
  project = var.project
  region  = var.region
}

resource "google_storage_bucket" "demo-bucket" {
  #it has to be unique across the whole GCP (so we can use our projectID for example)!!
  name          = var.gcs_bucket_name
  location      = var.location
  force_destroy = true

  lifecycle_rule {
    condition {
      #in days
      age = 1
    }
    action {
      #if the upload action a very big file that was split into chunks wasn't completed during one day - abort the upload
      type = "AbortIncompleteMultipartUpload"
    }
  }
}

resource "google_bigquery_dataset" "demo_dataset" {
  dataset_id = var.bq_dataset_name
  location   = var.location
}
```

```code
terraform plan
```









