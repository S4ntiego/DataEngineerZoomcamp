# Transfer data from VM to GCS using gsutil command
1. Start Jupyter Notebook in the data-engineering-zoomcamp/05-batch/code directory
2. Duplicate 06_spark_sql and rename to 09_spark_gcs

-m - multi-threaded / parallel, due to the big number of files (use all cpus/cores available on the virtual machine)
-r - recursive, copy the directory and all of its content, including subdirectories
gsutil - Python application that access Google Cloud from the command line
cp pq/ - copy pq folder from the data directory

```code terminal
cd data/
gsutil -m cp -r pq/ gs://mage-zoomcamp-adam/pq
```

# Transfer data from VM to GCS using Spark/Hadoop
Google Cloud Connector for Hadoop: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage
Remember that we need version 3.x, because we have downloaded Python version 3

```code terminal
cd ..
mkdir lib
cd lib/
gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar gcs-connector-hadoop3-2.2.5.jar
```

```python 09_spark_gcs
import pyspark
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.context import SparkContext

credentials_location = '/home/adam/keys/my-creds.json'


conf = SparkConf() \
    .setMaster('local[*]') \
    .setAppName('test') \
    .set("spark.jars", "./lib/gcs-connector-hadoop3-2.2.5.jar") \
    .set("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .set("spark.hadoop.google.cloud.auth.service.account.json.keyfile", credentials_location)

sc = SparkContext(conf=conf)

hadoop_conf = sc._jsc.hadoopConfiguration()

hadoop_conf.set("fs.AbstractFileSystem.gs.impl",  "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
hadoop_conf.set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
hadoop_conf.set("fs.gs.auth.service.account.json.keyfile", credentials_location)
hadoop_conf.set("fs.gs.auth.service.account.enable", "true")

spark = SparkSession.builder \
    .config(conf=sc.getConf()) \
    .getOrCreate()

df_green = spark.read.parquet('gs://mage-zoomcamp-adam/pq/green/*/*')

df_green.count()
```
